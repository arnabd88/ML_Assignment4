\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algo{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\newcommand\pseudoCode{\vspace{.10in}\textbf{PseudoCode: }}
\newcommand*{\perm}[2]{{}^{#1}\!P_{#2}}
\newcommand*{\comb}[2]{{}^{#1}\!C_{#2}}
%\pagestyle{fancyplain}
%\lhead{\textbf{\NAME\ (\UID)}}
%\chead{\textbf{Hw\HWNUM}}
%\rhead{CS 6350, \today}
\title{CS6350 - Homework/Assignment-4}
\author{Arnab Das(u1014840)}
\usepackage[utf8]{inputenc}
\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \pagenumbering{arabic}
  \newcommand\NAME{ARNAB DAS}
  \newcommand\UID{uxxxxxxx}
  \newcommand\HWNUM{4}
  
  %%---------- Question-1 -----------------
  \question{1}{PAC-Learning}
  \part{1.a} Rule-1 states that you are free to combine any of the parts as they are. Given, there are N available parts, then for every part there are two options - either the part is chosen or it is discarded. It is analogous to a N-bit vector, where each part corresponds to a bit in the vector. The decision of choosing the part can be thought of as setting 1 to that bit position while the choice of discarding a part is equivalent to seeting 0 to that bit position. For such a set-up the size of the hypotheses space is equal to the possible numbers that the n-bit vector can accomodate. Hence,\newline
  \hspace*{0.5cm} Size of hypotheses space by Rule-1 = $2^N$ (Answer). \newline
  
  \part{1.b} Here both Rule-1 and Rule-2 are followed. Rule-2 suggests that the original parts are allowed to be broken into two distinct pieces before using. Thus, in combination of Rule-1 and Rule-2, there will be now four choices for every part, that is , the part is not chosen, or the part is chosen as original without cut, or the part is cut and the first piece is chosen, or the part is cut and the second piece is chosen. For the analogy of the n-bit vector, each bit can now take 4 values instead of just two. Hence, \newline
  \hspace*{0.5cm} Size of hypotheses space by Rule-1 and 2 = $4^N$ (Answer). \newline
  
  \part{1.c} Number of available parts = 6. Hence, N = 6. \newline
  \hspace*{0.5cm} Size of hypotheses space = $|H|$ = $4^6$ \newline
  The allowed error,  $\epsilon$ = 0.01 \newline
  Given, the probability with which the robot wants to predict correctly within $\epsilon$ is 99\%. Then $(1-\delta)$ = 0.99. \newline
  \hspace*{0.5cm} Then, $\delta = $ 1 - 0.99 = 0.01 \newline
  Note that, in this case the learning is not agnostic, since the true concept is contained in the hypotheses space. Hence, we need to use the non-agnostic learning formula for finding the lower bound on the number of examples: \newline
  
  \begin{equation}
    m > \dfrac{1}{\epsilon}\bigg (  ln(|H|) + ln(\dfrac{1}{\delta})\bigg)
  \end{equation}
  Plugging in the values for $|H|, \epsilon , \delta$ we get the value of m as: \newline
  \[ m > 1292.293 \]
  Rounding off to the ceil:
  \[  m > 1293 \]
  (Answer)
  
  \part{2}
  Suppose H denotes the hypotheses space. Consider a hypothesis h $\in$ H. Let us define a random Variable `X' , such that:
  \[ X = 0 ; h(x)=c(x) \]
  \[ X = 1 ; h(x) \neq c(x) \]
  where x belongs to the distribution D over which the instance space is defined, and c the target concept. \newline
  If we select `m' random independent examples from the distribution D and the mark the outcomes as $X_{1},X_{2},\dots,X_{m}$, then $\dfrac{\sum X_{i}}{m}$ denotes the error fraction over the sample space. The \textbf {training error}, $error_{s}(h)$, is defines as the fraction of the training examples misclassified by the hypothesis h. Hence, \newline
  \begin{equation}
  error_{s}(h) = \dfrac{\sum X_{i}}{m}
  \end{equation}
  
  The \text {true error, or generalization error}, $error_{D}$ over the entire distribution D from which the examples are randomly drawn is defined as \newline
  \[ error_{D}(h) \equiv  \underset{x \in D}{P} [c(x) \neq h(x)]\]
    
    The expected value of a discrete random variable, is the probability weighted average of all possible values, that is over the entire distribution. Hence, the from the above definition of $error_{D}$, we can say it is the expected value of $error_{s}$, the training error.\newline
    
    For PAC learnability, we would like that the true error is not worse than $\epsilon$ plus the training error. Then we can characterize the event $(error_{D} - error_{s} > \epsilon)$ as a bad event and would like the probability of such an event be upperbounded by a small probability $\delta$. We can write the probability of the above event as following: \newline
    \[P[error_{D} > error_{s} + \epsilon]\]
    In this problem, we are given that the error term is a multiplicative terms relative to the training error, that is true error is no worse than  $(1 + \epsilon)error_{s}$. Then we can write the above as:
    \[P[error_{D} - (1+\epsilon)error_{s} > 0]\]
    Rearranging the terms:
    \[ P[error_{D} > (1+\epsilon)error_{s} ]\]
    \[ P[\dfrac{1}{1 + \epsilon}error_{D} > error_{s} ]\]
    \begin{equation}
     P\bigg [  error_{D}\bigg (  1 - \dfrac{\epsilon}{1 + \epsilon}  \bigg ) > error_{s}  \bigg ]
    \end{equation}
    
    Recalling the Chernoff bound, where for $X_{1},X_{2},\dots,X_{m}$ being the outcomes of m independent trials, and the probability of a $P[X_{i}=1]=p$ and $P[X_{i} = 0]=(1-p)$, then the expectation $E\bigg [  \dfrac{\sum X_{i}}{m}\bigg ]$=p, and the chernoff bound governs the probability that $\dfrac{\sum X_{i}}{m}$ will differ from p by some factor $0 < \gamma < 1$, as:
    \[ P\bigg [ \dfrac{\sum X_{i}}{m} > (1+\gamma)p \bigg ] \leq \exp(\dfrac{-mp\gamma^2}{3})\]
        \[ P\bigg [ \dfrac{\sum X_{i}}{m} < (1-\gamma)p \bigg ] \leq \exp(\dfrac{-mp\gamma^2}{2})\]
    
    Recalling equation(2) and the definition of true error, we see that equation(3) is in exact formation of the second chernoff bound described above such that \newline 
    $p=error_{D}$ and $\gamma = \dfrac{\epsilon}{1 + \epsilon}$. Since $0 < \epsilon < 1$, then $ 0 < \gamma < 1 $, which satisfies the condition on the form factor of the Chernoff bound. Thus, using the second chernoff bound in our problem, we get:
    \begin{equation}
    P\bigg [  error_{D}\bigg (  1 - \dfrac{\epsilon}{1 + \epsilon}  \bigg ) > error_{s}  \bigg ] \leq \exp(\dfrac{-m.error_{D}.\bigg (\dfrac{\epsilon}{1 + \epsilon}\bigg )^2}{2})
    \end{equation}
    
    The above was determined for a single hypethesis $h \in H$. For the entire hypotheses space the above equation has to be adjusted to :
    
    \begin{equation}
    P\bigg [  error_{D}\bigg (  1 - \dfrac{\epsilon}{1 + \epsilon}  \bigg ) > error_{s}  \bigg ] \leq |H|\exp \bigg (\dfrac{-m.error_{D}.\bigg (\dfrac{\epsilon}{1 + \epsilon}\bigg )^2}{2}\bigg )
    \end{equation}
    
    We use the result of equation(5) to determine the number of training examples required to reduce this probability of failure below a desired level of $\delta$. Hence: \newline
    \[ |H|\exp \bigg (\dfrac{-m.error_{D}.\bigg (\dfrac{\epsilon}{1 + \epsilon}\bigg )^2}{2}\bigg ) \leq \delta \]
    
    Taking natural logarithms on both sides and reaarnging the terms to single out m(number of examples for training), we get: \newline
    \begin{equation}
      m \geq \dfrac{2(1 + \epsilon)^2}{\epsilon^2 . error_{D}}\bigg (  ln|H| + ln|\dfrac{1}{\delta}| \bigg )
    \end{equation}
    
    (Answer).
  
  \question{2}{VC Dimensions}
  \part{1} Suppose we have a finite hypotheses space C. Let the instance space be 'X"  and the VC(C) defined over the space 'X" is n. This implies that C is able to shatter atmost a subset of n instances out of 'X'. To shatter n instances, the number oof hypotheses required will be atleast $2^n$. Hence, \newline
  \[|C| \geq 2^n\]
  \[=> \log_{2}|C| \geq n \times \log_{2}2\]
  \[=> n \leq \log_{2}|C| \]
  Hence, VC(C) $\leq \log_{2}|C|$ (Proved). \newline
  
  \part{2.a} 
    $H_{=k}^X$ = ${h \in {0,1}^X : |{x:h(x)=1}| = k}$
    that is, the set of all functions that assign the value 1 to exactly k elements.
    Consider 1 sample point. The adversary can mark this point as a `1' label or a `0'. The class defintiion says that for a given fixed `k', the hypotheses contained in this space are the ones that mark exactly k points as 1. Suppose k=1. The possible dichotomies of a single point is two, that is either the point is labeled as 1 or labeled as 0. Then, for k=1, it works for the dichotomy where the label is 1, but for label 0, it does not have a hypotheses in this class. The class for k=0, has a hypotheses for label=0, but it doesn't havs a hypotheses for the label being 1. Thus, for a specific fixed k, it cannot shatter a single point. Hence, \textbf { VC dimension of this class is 0}. (answer) \newline
    
    \part{2.b}
    $H_{\leq k}^X$ = ${ h \in {0,1}^X : |{x:h(x)=1}| \leq k or |{x:h(x)=0}| \leq k}$ \newline
    That is a class of functions for a given fixed k, there exists hypothesis that can label atmost k elements as 1 or a 0. Let us consider a few rudimentary cases to identify the formal pattern: \newline
    \textbf {case-1: single point - }  If this point is labeled 0, then there exists a $h \in H_{1}$ that can label a single point as 0, due to the fact $|{x:h(x)=0}| \leq 1$, and holds true for all $k \geq 1$. \newline
    If this point is labeled 1, then also there exists a $h \in H_{1}$ that can label a single point as 1 due to the fact $|{x:h(x)=1}| \leq 1$, and holds true for all $k \geq 1$. \newline
    \textbf {case-2: 2 points - } Consider 2 points. 2 points can have the following 4 labels: (0,0), (0,1), (1,0) and (1,1). In all 4 cases the $|x=1|$ is atmost 2, and hence satisfied by $h \in H_{2}:|x:h(x)=1| \leq 2$ and also $|x=0|$ is atmost 2 and hence satisfied by $h \in H_{2}:|x:h(x)=0| \leq 2$. \newline
    As the pattern is very evident that if the number of points, n, is less than or equal to k, then $H_{\leq k}^X$ can shatter the points.  \newline
    Now let us consider, that the number of points n is greater than k, say k+1, for a given fixed k. There exists a labelling where all the points are labelled as 1, such that $|x=1|$ = k+1. But the hypotheses class we have has hypothesis that can label \textbf {atmost} k points as 1. Hence, in this case we have a breakpoint and so the (k+1)  points cannot be shattered. \newline
    Hence, \textbf {VC dimension is k}. (answer) \newline

	\part{3} Instance space consisting of real numbers and a hypothesis space $H$ consisting of two disjoint intervals, defined by $[a,b]$ and $[c,d]$. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=25cm, height=20cm]{Prob3}
  \caption{Shattering of real number instance  space by two disjoint intervals}
  \end{figure}
  In figure-1, we describe the possible shatterings of set of points on the real number line. For 5 points we can show a labelling, where two disjoint intervals cannot shatter the set of points.Hence, \textbf {VC dimension is 4} . (Answer). \newline


  \part{4}
  Each example point in $R^2$ . A function $h \in H$, where H is the concept class, is specified by 2 parameters a and b. An example ${x_{1}, x_{2}}$ is labeled '+' if and only if $x_{1} + x_{2} \geq a$ and $x_{1} - x_{2} \leq b$, else labeled as '-'. So, it looks like the figure-2,  \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=14cm, height=8cm]{Prob4}
  \caption{Description of the function space}
  \end{figure}
  Fig-2(a), shows how the h changes with varying a and b. Note that $x_{1} + x_{2} = a$ and $x_{1} - x_{2} = b$ are perpendicular to each other. Hence, as we vary a and b, the lines do not rotate, but only move in parallel. Thus, all the functions correspond to fig2(b), that is a 45 degree axes rotated 4 quadrant structure, and that can be formed in any part of $R^2$ , with varying a and b. We have the quadrants as 1,2,3,4 for ease of explanation later. \newline
  Now, let us consider case by case for varying set of points. \newline
  \textbf {Case-1: Single Point - } A single point is fairly obvious that it can be shattered. If it is labeled `-', then we can always find pair of (a,b) such that the point falls in one of 2,3,4 quadrants. If it is marked as `+', then as well we can find a pair (a,b) such that the point falls in the 1 quadrant. \newline
  \textbf {Case-2: 2 points - } Figure(3) shows the set of two points that can be shattered with this hypotheses space. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=10cm, height=7cm]{Prob4a}
  \caption{Shattering two points}
  \end{figure}
  Fig(3)a shows for two points labeled (+,+) we can have a pair of (a,b) such that they lie in quadrant-1 of the 45 degree rotated 4 quadrant structure. Fig-3(b) analyses the case where labeling is (+,-). We show how the lines of 3(a) can be parallel shifted as the \textbf {blue} lines, to satisfy this labeling. Similarly, we show for the labelings of (-,+) and (-,-) in 3(c) and 3(d) respectively. Thus 2 points can be shattereds for this class. \newline

   \textbf {Case-3: 3 points -} Here, we strive to explain the different geomtries in which the points can be placed from the perspective of our hypothesis space. Figure(4) shows the different geomtries. The geometries that are symmetrical with respect to our hypetheses space are grouped together and only one of them will be considered. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=10cm, height=7cm]{Prob4b}
  \caption{Possible geometries of 3 points}
  \end{figure}

  In Figure(4) we show the counter-examples for all these geometries. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=15cm, height=10cm]{Prob4c}
  \caption{Counter Examples for each geometry of 3 points}
  \end{figure}
  For \textbf {geometry-1}, since the '-' labeled point is above the two '+' labeled points and midway along x1 for the '+' labeled points, hence the given labeling for geometry-1 is not satisfiable by any $h \in H$. \newline
  For \textbf {geometry-2}, For this setting we show two different labelings that cannot be simultaneously true. Since, every hypothesos with (a,b) are correspondingly parallel, this given pair of labelling violates it. \newline
  For \textbf {geometry-3}, Similarly, here also we show two labelling , that will conflict. \newline
  For \textbf {geometry-4,5,6,7}, For geometry 4,5,6,7, we show labellings, which does not have a feasible hypothesis, since the convex region of '+' will contain the '-' labeled point, contradicting the hypotheses space definition. \newline
  For \textbf {geometry-8,9,10}, Geometry-8,9,10 covers the colinear points where again we again give counter examples labeling that is not satisfiable by any of the hypotheses. Basically, for colinear points, if the non-adjacent points are oppositely labeled, then it is not satisfiable. \newline

  Since, we get breakpoint for 3 points, hence \textbf {VC dimension is 2}.(Answer)\newline

  

  
  
	
    
    


\end{document} 
