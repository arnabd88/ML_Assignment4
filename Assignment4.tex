\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{longtable}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algo{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\newcommand\pseudoCode{\vspace{.10in}\textbf{PseudoCode: }}
\newcommand*{\perm}[2]{{}^{#1}\!P_{#2}}
\newcommand*{\comb}[2]{{}^{#1}\!C_{#2}}
%\pagestyle{fancyplain}
%\lhead{\textbf{\NAME\ (\UID)}}
%\chead{\textbf{Hw\HWNUM}}
%\rhead{CS 6350, \today}
\title{CS6350 - Homework/Assignment-4}
\author{Arnab Das(u1014840)}
\usepackage[utf8]{inputenc}
\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \pagenumbering{arabic}
  \newcommand\NAME{ARNAB DAS}
  \newcommand\UID{uxxxxxxx}
  \newcommand\HWNUM{4}
  
  %%---------- Question-1 -----------------
  \question{1}{PAC-Learning}
  \part{1.a} Rule-1 states that you are free to combine any of the parts as they are. Given, there are N available parts, then for every part there are two options - either the part is chosen or it is discarded. It is analogous to a N-bit vector, where each part corresponds to a bit in the vector. The decision of choosing the part can be thought of as setting 1 to that bit position while the choice of discarding a part is equivalent to setting 0 to that bit position. For such a set-up the size of the hypotheses space is equal to the possible numbers that the n-bit vector can accomodate. If each part is considered as a variable $x_{i}$, such that $x_{i}$ denotes the presence of the feature while $\neg {x_{i}}$ denotes the absence of the feature, then we can define the function space as learning conjunctions. Each product made out of a set of parts constitutes a distinct hypothesis, hence the conjucntions must denote the presence or absence of a part. That is, suppose for a product with parts $x_{1}, x_{3}, x_{4}$, the distinct hypotheses cannot be $(x_{1} \wedge x_{3} \wedge x_{4})$ since this will be true even in presence of other parts like $x_{2}$. The correct one will be where the presence or absence of the parts are explicit, that is in this case, $(x_{1} \wedge \neg x_{2} \wedge x_{3} \wedge x_{4} \wedge \neg x_{5} \wedge \neg x_{6} \dots \wedge \neg x_{N})$. Thus for this conjucntion class the space will be $2^N$. \newline
  \hspace*{0.5cm} Size of hypotheses space by Rule-1 = $2^N$ (Answer). \newline
  
  \part{1.b} Here both Rule-1 and Rule-2 are followed. Rule-2 suggests that the original parts are allowed to be broken into two distinct pieces before using. Thus, in combination of Rule-1 and Rule-2, there will be now four choices for every part, that is , the part is not chosen, or the part is chosen as original without cut, or the part is cut and the first piece is chosen, or the part is cut and the second piece is chosen. For the analogy of the n-bit vector, each bit can now take 4 values instead of just two. Hence, \newline
  \hspace*{0.5cm} Size of hypotheses space by Rule-1 and 2 = $4^N$ (Answer). \newline
  
  \part{1.c} Number of available parts = 6. Hence, N = 6. \newline
  \hspace*{0.5cm} Size of hypotheses space = $|H|$ = $4^6$ \newline
  The allowed error,  $\epsilon$ = 0.01 \newline
  Given, the probability with which the robot wants to predict correctly within $\epsilon$ is 99\%. Then $(1-\delta)$ = 0.99. \newline
  \hspace*{0.5cm} Then, $\delta = $ 1 - 0.99 = 0.01 \newline
  Note that, in this case the learning is not agnostic, since the true concept is contained in the hypotheses space. Hence, we need to use the non-agnostic learning formula for finding the lower bound on the number of examples: \newline
  
  \begin{equation}
    m > \dfrac{1}{\epsilon}\bigg (  ln(|H|) + ln(\dfrac{1}{\delta})\bigg)
  \end{equation}
  Plugging in the values for $|H|, \epsilon , \delta$ we get the value of m as: \newline
  \[ m > 1292.293 \]
  Rounding off to the ceil:
  \[  m > 1293 \]
  (Answer)
  
  \part{2}
  Suppose H denotes the hypotheses space. Consider a hypothesis h $\in$ H. Let us define a random Variable `X' , such that:
  \[ X = 0 ; h(x)=c(x) \]
  \[ X = 1 ; h(x) \neq c(x) \]
  where x belongs to the distribution D over which the instance space is defined, and c the target concept. \newline
  If we select `m' random independent examples from the distribution D and the mark the outcomes as $X_{1},X_{2},\dots,X_{m}$, then $\dfrac{\sum X_{i}}{m}$ denotes the error fraction over the sample space. The \textbf {training error}, $error_{s}(h)$, is defines as the fraction of the training examples misclassified by the hypothesis h. Hence, \newline
  \begin{equation}
  error_{s}(h) = \dfrac{\sum X_{i}}{m}
  \end{equation}
  
  The \text {true error, or generalization error}, $error_{D}$ over the entire distribution D from which the examples are randomly drawn is defined as \newline
  \[ error_{D}(h) \equiv  \underset{x \in D}{P} [c(x) \neq h(x)]\]
    
    The expected value of a discrete random variable, is the probability weighted average of all possible values, that is over the entire distribution. Hence, the from the above definition of $error_{D}$, we can say it is the expected value of $error_{s}$, the training error.\newline
    
    For PAC learnability, we would like that the true error is not worse than $\epsilon$ plus the training error. Then we can characterize the event $(error_{D} - error_{s} > \epsilon)$ as a bad event and would like the probability of such an event be upperbounded by a small probability $\delta$. We can write the probability of the above event as following: \newline
    \[P[error_{D} > error_{s} + \epsilon]\]
    In this problem, we are given that the error term is a multiplicative terms relative to the training error, that is true error is no worse than  $(1 + \epsilon)error_{s}$. Then we can write the above as:
    \[P[error_{D} - (1+\epsilon)error_{s} > 0]\]
    Rearranging the terms:
    \[ P[error_{D} > (1+\epsilon)error_{s} ]\]
    \[ P[\dfrac{1}{1 + \epsilon}error_{D} > error_{s} ]\]
    \begin{equation}
     P\bigg [  error_{D}\bigg (  1 - \dfrac{\epsilon}{1 + \epsilon}  \bigg ) > error_{s}  \bigg ]
    \end{equation}
    
    Recalling the Chernoff bound, where for $X_{1},X_{2},\dots,X_{m}$ being the outcomes of m independent trials, and the probability of a $P[X_{i}=1]=p$ and $P[X_{i} = 0]=(1-p)$, then the expectation $E\bigg [  \dfrac{\sum X_{i}}{m}\bigg ]$=p, and the chernoff bound governs the probability that $\dfrac{\sum X_{i}}{m}$ will differ from p by some factor $0 < \gamma < 1$, as:
    \[ P\bigg [ \dfrac{\sum X_{i}}{m} > (1+\gamma)p \bigg ] \leq \exp(\dfrac{-mp\gamma^2}{3})\]
        \[ P\bigg [ \dfrac{\sum X_{i}}{m} < (1-\gamma)p \bigg ] \leq \exp(\dfrac{-mp\gamma^2}{2})\]
    
    Recalling equation(2) and the definition of true error, we see that equation(3) is in exact formation of the second chernoff bound described above such that \newline 
    $p=error_{D}$ and $\gamma = \dfrac{\epsilon}{1 + \epsilon}$. Since $0 < \epsilon < 1$, then $ 0 < \gamma < 1 $, which satisfies the condition on the form factor of the Chernoff bound. Thus, using the second chernoff bound in our problem, we get:
    \begin{equation}
    P\bigg [  error_{D}\bigg (  1 - \dfrac{\epsilon}{1 + \epsilon}  \bigg ) > error_{s}  \bigg ] \leq \exp(\dfrac{-m.error_{D}.\bigg (\dfrac{\epsilon}{1 + \epsilon}\bigg )^2}{2})
    \end{equation}
    
    The above was determined for a single hypethesis $h \in H$. For the entire hypotheses space the above equation has to be adjusted to :
    
    \begin{equation}
    P\bigg [  error_{D}\bigg (  1 - \dfrac{\epsilon}{1 + \epsilon}  \bigg ) > error_{s}  \bigg ] \leq |H|\exp \bigg (\dfrac{-m.error_{D}.\bigg (\dfrac{\epsilon}{1 + \epsilon}\bigg )^2}{2}\bigg )
    \end{equation}
    
    We use the result of equation(5) to determine the number of training examples required to reduce this probability of failure below a desired level of $\delta$. Hence: \newline
    \[ |H|\exp \bigg (\dfrac{-m.error_{D}.\bigg (\dfrac{\epsilon}{1 + \epsilon}\bigg )^2}{2}\bigg ) \leq \delta \]
    
    Taking natural logarithms on both sides and reaarnging the terms to single out m(number of examples for training), we get: \newline
    \begin{equation}
      m \geq \dfrac{2(1 + \epsilon)^2}{\epsilon^2 . error_{D}}\bigg (  ln|H| + ln|\dfrac{1}{\delta}| \bigg )
    \end{equation}
    
    (Answer).
  
  \question{2}{VC Dimensions}
  \part{1} Suppose we have a finite hypotheses space C. Let the instance space be 'X"  and the VC(C) defined over the space 'X" is n. This implies that C is able to shatter atmost a subset of n instances out of 'X'. To shatter n instances, the number oof hypotheses required will be atleast $2^n$. Hence, \newline
  \[|C| \geq 2^n\]
  \[=> \log_{2}|C| \geq n \times \log_{2}2\]
  \[=> n \leq \log_{2}|C| \]
  Hence, VC(C) $\leq \log_{2}|C|$ (Proved). \newline
  
  \part{2.a} 
    $H_{=k}^X$ = ${h \in {0,1}^X : |{x:h(x)=1}| = k}$ . It has two cases, analyzed below. \newline
    \textbf {case-1}: Suppose, your current set of instance is of size $n$ and there are $(k-1)$ remaining points in the instance space. To meet the criterian, that the hypotheses class requires exactly 'k' elements to be labeled as '1', we can mark the 'k-1' points as '1', but still require 1 more point from the current set of instances under analysis. However, this set can have an all 0 labeling required to be satisfied, which will not be satisfied by the hypotheses class. In this case thus $n = |X| - (k-1)$ . If we consider, the remaining points were greater than equal to k, in that case, for the extreme labelings of the set of instances, we can always fit the exact k points in this remaining part for the all 0 labelling, and in other cases will be able to distribute the exact k 1's amongst all the points such that every label of our current instance set can be satisfied, except in another case disussed in case-2. Thus, the VC is greater than equal to $|X| - k$ \newline
    \textbf {case-2}: Suppose you have (k+1) points under consideration in your subset of instances, then for the labeling of all 1's , we have one more point to satisfy than supported by the hypotheses class. In this case thus $n = k+1$. \newline

    Thus depending on the distance/difference between $|X|$ and k, either of the cases of case-1 or case-2 will drive the VC dimension. If $|X| >> k$, then the value of k will drive the VC dimension, else if $|X| < 2k+1$ or $|X| - k < k+1$, then $|X|-k$ drives the VC dimension.
    Thus for a finite instance space, the VC dimension will be 1 less than the minima of the n's described above. Hence: \newline
    \[ VC = min(|X| - (k) , k)\]
    (Answer).
    
    \part{2.b}
    $H_{\leq k}^X$ = ${ h \in {0,1}^X : |{x:h(x)=1}| \leq k or |{x:h(x)=0}| \leq k}$ \newline
    Consider the case that your current instance set is of size $(2k+1)$. Then for the labelling where (k+1) points are labeled as '1' and k points are labeled as 0. In this case, the remaining points, $|X|-(2k+1)$, will have to be labeled as 1 such that the constraint of $|{x:h(x)=0}| \leq k$ satisfies. Similarly, if you have (k+1) points labeled as '0' and k points are labeled as '1', then the remaining points, $|X| - (2k+1)$, will have to be labeled as 0 such that the constraint of $|{x:h(x)=1}| \leq k$ satisfies. For any other combinations of labeling for these (2k+1) set, either $|{x:h(x)=1}| < k$ or $|{x:h(x)=0}| < k$ will satisfy which still satisfies the hypotheses space conditions. Thus, we see the \textbf {VC is atleast (2k+1)}. Then if we consider, (2k+2) instance size, then there will be a valid labeling of (k+1) 1's and (k+1) 0's. Since the count of 1's and 0's exceed k, this is not satisfiable by any of the hypothesis in the hypotheses class. Hence, it breaks for 2k+2 points. \textbf {Thus, considering $|X| \geq 2k+1$, the VC will be (2k+1)}. (Answer). \newline
    If, $|X| < 2k+1$, then as we can shatter at most (2k+1) points, hence, the VC will be $|X|$. (Answer). \newline


  \part{3}
  \begin{figure}[H]
   \centering
  \includegraphics[width=25cm, height=16cm]{Prob3}
  \caption{Shattering of real number instance  space by two disjoint intervals}
  \end{figure}
  Instance space consisting of real numbers and a hypothesis space $H$ consisting of two disjoint intervals, defined by $[a,b]$ and $[c,d]$. \newline
  In figure-1, we describe the possible shatterings of set of points on the real number line. For 5 points we can show a labelling, where two disjoint intervals cannot shatter the set of points.Hence, \textbf {VC dimension is 4} . (Answer). \newline


  \part{4}
  Each example point in $R^2$ . A function $h \in H$, where H is the concept class, is specified by 2 parameters a and b. An example ${x_{1}, x_{2}}$ is labeled '+' if and only if $x_{1} + x_{2} \geq a$ and $x_{1} - x_{2} \leq b$, else labeled as '-'. So, it looks like the figure-2,  \newline
  \begin{figure}[H]
   \centering
  \includegraphics[width=14cm, height=8cm]{Prob4}
  \caption{Description of the function space}
  \end{figure}
  Fig-2(a), shows how the h changes with varying a and b. Note that $x_{1} + x_{2} = a$ and $x_{1} - x_{2} = b$ are perpendicular to each other. Hence, as we vary a and b, the lines do not rotate, but only move in parallel. Thus, all the functions correspond to fig2(b), that is a 45 degree axes rotated 4 quadrant structure, and that can be formed in any part of $R^2$ , with varying a and b. We have the quadrants as 1,2,3,4 for ease of explanation later. \newline
  Now, let us consider case by case for varying set of points. \newline
  \textbf {Case-1: Single Point - } A single point is fairly obvious that it can be shattered. If it is labeled `-', then we can always find pair of (a,b) such that the point falls in one of 2,3,4 quadrants. If it is marked as `+', then as well we can find a pair (a,b) such that the point falls in the 1 quadrant. \newline
  \textbf {Case-2: 2 points - } Figure(3) shows the set of two points that can be shattered with this hypotheses space. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=10cm, height=7cm]{Prob4a}
  \caption{Shattering two points}
  \end{figure}
  Fig(3)a shows for two points labeled (+,+) we can have a pair of (a,b) such that they lie in quadrant-1 of the 45 degree rotated 4 quadrant structure. Fig-3(b) analyses the case where labeling is (+,-). We show how the lines of 3(a) can be parallel shifted as the \textbf {blue} lines, to satisfy this labeling. Similarly, we show for the labelings of (-,+) and (-,-) in 3(c) and 3(d) respectively. Thus 2 points can be shattereds for this class. \newline

   \textbf {Case-3: 3 points -} Here, we strive to explain the different geomtries in which the points can be placed from the perspective of our hypothesis space. Figure(4) shows the different geomtries. The geometries that are symmetrical with respect to our hypetheses space are grouped together and only one of them will be considered. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=10cm, height=7cm]{Prob4b}
  \caption{Possible geometries of 3 points}
  \end{figure}

  In Figure(4) we show the counter-examples for all these geometries. \newline
  \begin{figure}[h!]
   \centering
  \includegraphics[width=15cm, height=10cm]{Prob4c}
  \caption{Counter Examples for each geometry of 3 points}
  \end{figure}
  For \textbf {geometry-1}, since the '-' labeled point is above the two '+' labeled points and midway along x1 for the '+' labeled points, hence the given labeling for geometry-1 is not satisfiable by any $h \in H$. \newline
  For \textbf {geometry-2}, For this setting we show two different labelings that cannot be simultaneously true. Since, every hypothesos with (a,b) are correspondingly parallel, this given pair of labelling violates it. \newline
  For \textbf {geometry-3}, Similarly, here also we show two labelling , that will conflict. \newline
  For \textbf {geometry-4,5,6,7}, For geometry 4,5,6,7, we show labellings, which does not have a feasible hypothesis, since the convex region of '+' will contain the '-' labeled point, contradicting the hypotheses space definition. \newline
  For \textbf {geometry-8,9,10}, Geometry-8,9,10 covers the colinear points where again we again give counter examples labeling that is not satisfiable by any of the hypotheses. Basically, for colinear points, if the non-adjacent points are oppositely labeled, then it is not satisfiable. \newline

  Since, we get breakpoint for 3 points, hence \textbf {VC dimension is 2}.(Answer)\newline

 \part{5} 
 Let two hypothesis classes $H_{1}$ and $H_{2}$ satisfy $H_{1} \subseteq H_{2}$. \newline
 Suppose we find a set of instances of size $d_{1}$ that $H_{1}$ is able to shatter and a size $d_{2}=d_{1}+1$, that $H_{1}$ cannot shatter. Then \newline
 \[ d1 \leq VC(H1) < d2 \]
 Hence,
\[ VC(H1) = d1 \]
 Since , $H_{1} \subseteq H_{2}$, we can write:  $H_{2} = H_{1} \cup \delta h$. \newline
 Since, $H_{2}$ contains all the hypotheses of $H_{1}$, so $H_{2}$ can \textbf {atleast} shatter the subset of instances that $H_{1}$ was able to shatter. Hence, for d1 set of instances, $H_{2}$ will be able to shatter using the same set of hypotheses $H_{1}$ had to shatter $d_{1}$. Hence: \newline
 \[ VC(H_{2}) \geq d1 = VC(H_{1}) \]
 \begin{equation}
  VC(H_{2}) \geq VC(H_{1})
 \end{equation}
 (Proved).\newline


 \question{3}{AdaBoost}
 Using the formulae: \newline
 (1) $\epsilon_{t} = \dfrac{1}{2} - \dfrac{1}{2}(\sum_{i=1}^m D_{t}(i)y_{i}h(x_{i}))$ \newline
 (2) $D_{t+1}(i) = \dfrac{D_{t}(i)}{z_{t}}\exp(-\alpha_{t}y_{i}h_{t}(x_{i}))$ \newline
 (3) $\alpha_{t} = ln(\dfrac{1-\epsilon_{t}}{\epsilon_{t}})$ \newline
 \textbf {Given for first iteration}: \newline
 h = $h_{a}(x)$ = sgn($x_{1}$), $\epsilon_{1}=1/4$, $\alpha_{1}=\dfrac{ln3}{2}$ \newline
 Initialize $D_{1}$ = $\bigg ( \dfrac{1}{4}, \dfrac{1}{4}, \dfrac{1}{4}, \dfrac{1}{4},\bigg )$ \newline
 Evaluating $D_{2}^\prime (without-normalization)$ = $\bigg ( \dfrac{\sqrt[2]{3}}{4}, \dfrac{1}{4\sqrt[2]{3}}, \dfrac{1}{4\sqrt[2]{3}}, \dfrac{1}{4\sqrt[2]{3}} \bigg )$ \newline
 $Z_{1}$ = $\sum_{i=1}^4 D_{2}^\prime(i)$ = $\dfrac{\sqrt[2]{3}}{2}$ \newline
 Hence, Normalized $D_{2}$ = $\bigg ( \dfrac{1}{2}, \dfrac{1}{6}, \dfrac{1}{6}, \dfrac{1}{6} \bigg )$ \newline
 \textbf {Starting second iteration} \newline
 \textbf {Evaluating $\epsilon$ corresponding to the remaining hypotheses} \newline
 $\epsilon_{2,h_{b}}$ = $\dfrac{1}{6}$ \newline
 $\epsilon_{2,h_{c}}$ = $\dfrac{1}{2}$ \newline
 $\epsilon_{2,h_{d}}$ = $\dfrac{1}{6}$ \newline
 Since both $\epsilon_{2,h_{b}}$ and $\epsilon_{2,h_{d}}$ are lowest and better than chance, we can pick any of $h_{b}$ or $h_{d}$ . So Choosing $h_{b}$ for round 2, we evaluate $\alpha_{2} = \dfrac{ln5}{2}$ \newline
 Hence the data for Round-2 is: \newline
 h = $h_{b}$ = sgn(x-2) ; $\epsilon_{2} = \epsilon_{2,h_{b}} = \dfrac{1}{6}$,  $D_{2}$ = $\bigg ( \dfrac{1}{2}, \dfrac{1}{6}, \dfrac{1}{6}, \dfrac{1}{6} \bigg )$ \newline
 Evaluating $D_{3}^\prime(without-normalization)$ = $\bigg (  \dfrac{1}{2\sqrt[2]{5}}, \dfrac{\sqrt[2]{5}}{6}, \dfrac{1}{6\sqrt[2]{5}}, \dfrac{1}{6\sqrt[2]{5}}  \bigg )$ \newline
 $Z_{2}$ = $\dfrac{\sqrt[2]{5}}{3}$ \newline
 Hence, Normalized $D_{3}$ = $\bigg (  \dfrac{3}{10}, \dfrac{1}{2} , \dfrac{1}{10}, \dfrac{1}{10} \bigg )$ \newline
 \textbf {Starting third iteration} \newline
 \textbf {Evaluating $\epsilon$ corresponding to the remaining hypotheses} \newline
 $\epsilon_{3,h_{c}} = \dfrac{7}{10}$ \newline
 $\epsilon_{3,h_{d}} = \dfrac{1}{10}$ \newline
 Since $\epsilon_{3,h_{d}}$ is lowest and better than chance, hence choosing $h_{d}$ for round 3, and evaluating $\alpha_{3} = ln(3)$ \newline
 Hence, the data for Round-3 is: \newline
 h = $h_{d}$ = $-sgn(x_{2})$ ; $\epsilon_{3} = \epsilon_{3,h_{d}} = \dfrac{1}{10}$, $D_{3}$ = $\bigg (  \dfrac{3}{10}, \dfrac{1}{2} , \dfrac{1}{10}, \dfrac{1}{10} \bigg )$ \newline
 Evaluating $D_{4}^\prime(without-normalization)$ = $\bigg ( \dfrac{1}{10}, \dfrac{1}{6}, \dfrac{3}{10}, \dfrac{1}{30} \bigg )$ \newline
 $Z_{3}$ = $\dfrac{3}{5}$ \newline
 Hence, Normalized $D_{4}$ = $\bigg ( \dfrac{1}{6} , \dfrac{5}{18}, \dfrac{1}{2}, \dfrac{1}{18} \bigg )$ \newline
 \textbf {Starting fourth iteration} \newline
 \textbf {Evaluating $\epsilon$ corresponding to the remaining hypotheses} \newline
 $\epsilon_{4,h_{c}} = \dfrac{5}{6}$ \newline
 Since, $\epsilon_{4,h_{c}}$ is worse than chance, we \textbf {discard this hypotheses $h_{d}$}. \newline
 Hence, our final combined function is :\newline
 \begin{equation}
    H_{final} = sgn( (ln(\sqrt[2]{3}))sgn(x_{1}) + (ln(\sqrt[2]{5}))sgn(x-2) - (ln3)sgn(x_{2}))
 \end{equation}
 The final table is given in Table-2: \newline

\begin{longtable}{c|c|c|c|c}
	\caption{Table-Round1: $h_{a}(x), \epsilon_{1} = \dfrac{1}{4}, \alpha_{1}=\dfrac{ln3}{2}, Z_{1} = \dfrac{\sqrt[2]{3}}{2}$} \\
  %\centering
  %\begin{tabular}{c c  c c c c}
  \hline\hline
	  $X=[x_{1},x_{2}]$ & $y_{i}$ & $D_{1}$ & $D_{1}(i)y_{i}h_{t}(x_{i})$ & $D_{2}$ \\[0.5ex]
  \hline
  (1,-1) & -1 & $\dfrac{1}{4}$ & $-\dfrac{1}{4}$ & $\dfrac{1}{2}$ \\
  (1,-1) & +1 & $\dfrac{1}{4}$ & $\dfrac{1}{4}$ & $\dfrac{1}{6}$ \\
  (-1,-1) & -1 & $\dfrac{1}{4}$ & $\dfrac{1}{4}$ & $\dfrac{1}{6}$ \\
	  (-1,1) & -1 & $\dfrac{1}{4}$ & $\dfrac{1}{4}$ & $\dfrac{1}{6}$ \\ [0.5ex]
  %1 & 3 & -4.002588
  %\end{tabular}
  %\label{table:nonlin}
  \end{longtable}

\begin{longtable}{c|c|c|c|c}
	\caption{Table-Round2: $h_{b}(x), \epsilon_{2} = \dfrac{1}{6}, \alpha_{2}=\dfrac{ln5}{2}, Z_{2} = \dfrac{\sqrt[2]{5}}{3}$} \\
  %\centering
  %\begin{tabular}{c c  c c c c}
  \hline\hline
	  $X=[x_{1},x_{2}]$ & $y_{i}$ & $D_{2}$ & $D_{2}(i)y_{i}h_{t}(x_{i})$ & $D_{3}$ \\[0.5ex]
  \hline
 	  (1,-1) & -1 & $\dfrac{1}{2}$ & $\dfrac{1}{2}$ & $\dfrac{3}{10}$ \\
	  (1,-1) & +1 & $\dfrac{1}{6}$ & $\dfrac{-1}{6}$ & $\dfrac{1}{2}$ \\
	  (-1,-1) & -1 & $\dfrac{1}{6}$ & $\dfrac{1}{6}$ & $\dfrac{1}{10}$ \\
	  (-1,1) & -1 & $\dfrac{1}{6}$ & $\dfrac{1}{6}$ & $\dfrac{1}{10}$ \\[0.5ex]
  %1 & 3 & -4.002588
  %\end{tabular}
  %\label{table:nonlin}
  \end{longtable}

\begin{longtable}{c|c|c|c|c}
	\caption{Table-Round3: $h_{d}(x), \epsilon_{3} = \dfrac{1}{10}, \alpha_{3}=ln3, Z_{3} = \dfrac{3}{5}$} \\
  %\centering
  %\begin{tabular}{c c  c c c c}
  \hline\hline
	  $X=[x_{1},x_{2}]$ & $y_{i}$ & $D_{3}$ & $D_{3}(i)y_{i}h_{t}(x_{i})$ & $D_{4}$ \\[0.5ex]
  \hline
	  (1,-1) & -1 & $\dfrac{3}{10}$ & $\dfrac{3}{10}$ & $\dfrac{1}{6}$\\
	  (1,-1) & +1 & $\dfrac{1}{2}$ & $\dfrac{1}{2}$ & $\dfrac{5}{18}$\\
	  (-1,-1) & -1 & $\dfrac{1}{10}$ & $\dfrac{-1}{10}$ & $\dfrac{1}{2}$\\
	  (-1,1) & -1 & $\dfrac{1}{10}$ & $\dfrac{1}{10}$ & $\dfrac{1}{18}$ \\[0.5ex]
  %1 & 3 & -4.002588
  %\end{tabular}
  %\label{table:nonlin}
  \end{longtable}

  $\epsilon_{4,h_{c}}$ = $\dfrac{5}{6}$, worse than chance, hence discarded. \newline
  \textbf {Final Hypothesis}, $H_{final}(x) =  sgn( (ln(\sqrt[2]{3}))sgn(x_{1}) + (ln(\sqrt[2]{5}))sgn(x-2) - (ln3)sgn(x_{2}))$ (Answer).


  
  
	
    
    


\end{document} 
